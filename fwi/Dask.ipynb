{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Introduction </h1>\n",
    "\n",
    "This notebook demonstrates how to scale an implemenation of Full Waveform Inversion using multiple FPGAs with bitstreams containing one or multiple copied compute units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define experiment parameters: </h3>\n",
    "    \n",
    "PLATFORM => One of the two supported platforms by the driver (alveo/zynq-iodma)\n",
    "\n",
    "XCLBIN_PATH_ => The path to the .xclbin file\n",
    "\n",
    "DEVICE_NAME_DEFAULT => Default name for the FPGA device if one not provided via command line args\n",
    "\n",
    "XRT_ENV_PATH => The path to the setup file that can be used to source the XRT environment\n",
    "\n",
    "SPAWN_PATH => The path where the dask workers are spawned  \n",
    "\n",
    "DIR_PATH => Path to the directory containing the FWI input files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "XCLBIN_PATH_8000 = \"bitstreams/8000_100_1CU/8000_100_1cu.xclbin\"\n",
    "\n",
    "XRT_ENV_PATH = \"/opt/xilinx/xrt/setup.sh\"\n",
    "DEVICE_NAME_DEFAULT=\"xilinx_u280_xdma_201920_3\"\n",
    "SPAWN_PATH = \"/home/<username>/octoray/fwi/\"\n",
    "DIR_PATH_FWI = \"default/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster configuration\n",
    "The cluster configuration is used by OctoRay to set up the cluster. The configuration is identical to the Dask SSH Cluster found at: https://docs.dask.org/en/stable/deploying-ssh.html  \n",
    "In addition, the **xrt** and **dir** keywords can be used to specify the path to the XRT setup file and the worker's spawn paths.  \n",
    "The **overlay** keyword is used to specify the bitstream used. By passing a list of bitstreams it is possible to configure different bit streams on different hosts. The list must be the same length as the number of hosts.  \n",
    "  \n",
    "In this example a configuration is specified for two hosts. The first host uses a single instance bitstream and therefore a single worker is specified in the worker options. As the second host uses a double bitstream, the number of workers is specifed to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "cluster_config = {\n",
    "    \"scheduler\":\"10.1.212.130\",\n",
    "    \"hosts\":[\"10.1.212.130\"],\n",
    "    \"connect_options\":{\"port\":22,\"xrt\":XRT_ENV_PATH,\"dir\":SPAWN_PATH},\n",
    "    \"worker_options\":{\"nthreads\":0,\"n_workers\":1,\"preload\":\"pynqimport.py\",\"nanny\":\"0\",\"memory_limit\":0},\n",
    "    \"scheduler_options\":{\"port\":8786},\n",
    "    \"worker_class\":\"distributed.Worker\",\n",
    "    \"overlay\": XCLBIN_PATH_8000,\n",
    "}\n",
    "\n",
    "with open(\"cluster_config.json\",\"w\") as f:\n",
    "    json.dump(cluster_config,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define the worker method </h3> \n",
    "\n",
    "Here, we define the Python method which will be executed on each of the Dask workers. This function calls the driver using the data partition it receives, and returns the output data (along with some performance statistics) to the caller (the OctoRay client). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(grid_data,kernel,id):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import psutil\n",
    "    import time\n",
    "    from pynq import Overlay, allocate, Device, lib\n",
    "\n",
    "    from FWIDriver import FWI\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set up the configuration\n",
    "    cu = kernel[\"instance_id\"]\n",
    "    config = kernel[\"config\"]\n",
    "    path = kernel[\"path_to_bitstream\"]\n",
    "    gridsize = config[\"grid\"]\n",
    "    \n",
    "    resolution = config[\"Freq\"][\"nTotal\"] * config[\"nSources\"] * config[\"nReceivers\"]\n",
    "    config[\"tolerance\"] = 9.99*10**-7\n",
    "    config[\"max\"] = 1000\n",
    "    \n",
    "    acceleration = True\n",
    "\n",
    "    if acceleration:\n",
    "        \n",
    "        # Load the overlay\n",
    "        devices = Device.devices\n",
    "        ol = Overlay(path, download=False, device=devices[0])\n",
    "\n",
    "        # Allocate the buffers\n",
    "        A = allocate(shape=(resolution,gridsize), dtype=np.complex64, target=getattr(ol,kernel[\"functions\"][0][\"dotprod_\"+str(cu)][0]))\n",
    "        B = allocate(shape=(gridsize,), dtype=np.float32, target=getattr(ol,kernel[\"functions\"][0][\"dotprod_\"+str(cu)][1]))\n",
    "        C = allocate(shape=(resolution,), dtype=np.complex64, target=getattr(ol,kernel[\"functions\"][0][\"dotprod_\"+str(cu)][2]))\n",
    "\n",
    "        D = allocate(shape=(resolution,gridsize), dtype=np.complex64,  target=getattr(ol,kernel[\"functions\"][1][\"update_\"+str(cu)][0]))\n",
    "        E = allocate(shape=(resolution,),dtype=np.complex64,  target=getattr(ol,kernel[\"functions\"][1][\"update_\"+str(cu)][1]))\n",
    "        F = allocate(shape=(gridsize), dtype=np.complex64, target=getattr(ol,kernel[\"functions\"][1][\"update_\"+str(cu)][2]))\n",
    "\n",
    "        # set up the kernel IP's\n",
    "        dotprod = getattr(ol,\"dotprod_\"+str(cu))\n",
    "        update = getattr(ol,\"update_\"+str(cu))\n",
    "\n",
    "        \n",
    "        # Execute the Full Waveform Inversion algorithm\n",
    "        fwi = FWI(A,B,C,D,E,F,dotprod,update,config,resolution,gridsize,acceleration)\n",
    "    else:\n",
    "        fwi = FWI(config=config,resolution=resolution,gridsize=gridsize,acceleration=acceleration)\n",
    "\n",
    "    fwi.pre_process(grid_data)\n",
    "\n",
    "    # reconstruct the grid by performing Full Wavefrom Inversion\n",
    "    chi = fwi.reconstruct()\n",
    "\n",
    "\n",
    "    if acceleration:\n",
    "        # free all the buffers\n",
    "        A.freebuffer()\n",
    "        B.freebuffer()\n",
    "        C.freebuffer()\n",
    "        D.freebuffer()\n",
    "        E.freebuffer()\n",
    "        F.freebuffer()\n",
    "        ol.free()\n",
    "        \n",
    "    # Return statistics and results from FWI\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    dict_t = {\n",
    "    \"index\": id,\n",
    "    \"cu\": cu,\n",
    "    \"dot\": fwi.model.dot_time,\n",
    "    \"upd\": fwi.inverse.updtime,\n",
    "    \"time\": total_time,\n",
    "    }\n",
    "    \n",
    "    return dict_t\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Set up Octoray </h2>\n",
    "\n",
    "Set up the Octoray framework. A user can pass either a dict or a config file containing a dict and specifiy if the scheduler and workers need to be set up or are already instantiated manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OctoRay with client ip: 10.1.212.130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.deploy.ssh - INFO - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - Clear task state\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO -   Scheduler at:   tcp://10.1.212.130:8786\n",
      "distributed.deploy.ssh - INFO - distributed.diskutils - INFO - Found stale lock file and directory '/home/<username>/octoray/fwi/dask-worker-space/worker-grtmx6m_', purging\n",
      "distributed.deploy.ssh - INFO - distributed.utils - INFO - Reload module pynqimport from .py file\n",
      "distributed.deploy.ssh - INFO - distributed.preloading - INFO - Import preload module: pynqimport.py\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:   tcp://10.1.212.130:46747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting until workers are set up on remote machines...\n",
      "Current amount of workers: 1\n"
     ]
    }
   ],
   "source": [
    "from octoray import Octoray\n",
    "\n",
    "octoray = Octoray(ssh_cluster=True,cluster_config=cluster_config)\n",
    "\n",
    "octoray.create_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': 1, 'cu': 1, 'dot': 1.0364792346954346, 'upd': 0.2154538631439209, 'time': 3.294934034347534}]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "# Load in data and config settings\n",
    "original_data = []\n",
    "with open(DIR_PATH_FWI+\"input/\"+\"10x10_100\"+\".txt\") as f:\n",
    "    for l in f:\n",
    "        original_data.append(float(l))\n",
    "        \n",
    "\n",
    "fwi_config = None\n",
    "with open(DIR_PATH_FWI+\"input/GenericInput.json\") as f:\n",
    "    fwi_config = json.load(f)\n",
    "\n",
    "\n",
    "#set specific configurations for different types of kernels\n",
    "fwi_config[\"grid\"] = 100\n",
    "\n",
    "single_cu_config = fwi_config\n",
    "single_cu_config[\"ngrid\"][\"x\"]=10\n",
    "single_cu_config[\"ngrid\"][\"z\"]=10\n",
    "double_cu_config = copy.deepcopy(fwi_config)\n",
    "\n",
    "single_cu_config[\"Freq\"][\"nTotal\"]=20\n",
    "single_cu_config[\"nSources\"]=20\n",
    "single_cu_config[\"nReceivers\"]=20\n",
    "\n",
    "double_cu_config[\"Freq\"][\"nTotal\"]=10\n",
    "double_cu_config[\"nSources\"]=20\n",
    "double_cu_config[\"nReceivers\"]=20\n",
    "\n",
    "fwi_kernels = []\n",
    "\n",
    "# Configure the kernels by specifying the path to the bitstream, number of compute units, batchsize per compute unit and the function names and variables with their respective memory banks.\n",
    "fwi_kernels.append(octoray.create_kernel(XCLBIN_PATH_8000,1,int(100),[[{\"dotprod_1\":[\"DDR0\",\"DDR0\",\"DDR0\"]},{\"update_1\":[\"DDR1\",\"DDR1\",\"DDR1\"]}]],single_cu_config,host=\"10.1.212.130\",device=DEVICE_NAME_DEFAULT))\n",
    "\n",
    "# In the case of multiple instances, subdivide the kernels over the number of instances\n",
    "kernels_split = octoray.split_kernels(fwi_kernels)    \n",
    "\n",
    "# Divide the data set over the kernels based on batch size per instance\n",
    "data_split = octoray.split_data(original_data,kernels_split)\n",
    "\n",
    "\n",
    "# Launch the tasks after scattering the data and kernels to the correct workers\n",
    "result = octoray.execute_hybrid(execute_function,data_split,kernels_split)\n",
    "\n",
    "# Reorder the response based on the original input order\n",
    "result.sort(key = lambda result: result['index'])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3> Graceful shutdown for OpenSSH version >= 7.9 </h3>\n",
    " \n",
    "Doesn't work on XACC yet... (UNTESTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "octoray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ungraceful shutdown for OpenSSH version < 7.9 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await octoray.fshutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
